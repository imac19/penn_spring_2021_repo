# -*- coding: utf-8 -*-
"""hw6_text_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16i3LOAN9up7AGdlbW84pbkGy8FJTMoic
"""

import string
import random
# =============================================================================
# import re
# from google.colab import drive
# =============================================================================
import torch
import torch.nn as nn
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np 

# =============================================================================
# drive.mount('/content/gdrive')
# =============================================================================

all_letters = string.printable

n_characters = len(all_letters)

def get_text_string(read_location):
    word_string = open(read_location).read()

    return word_string

def get_random_start_letter():
    string_rand = string.printable[36:62]
    rand_index = random.randint(0, len(string_rand)-1)
    
    return string_rand[rand_index]

def random_chunk(text_string, file_len, chunk_len=200):
    start_index = random.randint(0, file_len - chunk_len)
    end_index = start_index + chunk_len + 1
    string_chunk = text_string[start_index:end_index]

    return string_chunk

def string_to_tensor(string):
    tensor = torch.zeros(len(string)).long()
    for char_index in range(0, len(string)):
        try:
            tensor[char_index] = all_letters.index(string[char_index])
        except:
            tensor[char_index] = all_letters.index('A')

    return Variable(tensor)

def get_random_training_set(text_string, file_len, chunk_len=200):
    chunk = random_chunk(text_string, file_len, chunk_len)
    input = string_to_tensor(chunk[:-1])
    target = string_to_tensor(chunk[1:])
    
    return input, target 

def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):
    hidden = decoder.init_hidden()
    prime_input = string_to_tensor(prime_str)
    predicted = prime_str

    for p in range(len(prime_str) - 1):
        _, hidden = decoder(prime_input[p], hidden)
    inp = prime_input[-1]
    
    for p in range(predict_len):
        output, hidden = decoder(inp, hidden)
        
        output_dist = output.data.view(-1).div(temperature).exp()
        top_i = torch.multinomial(output_dist, 1)[0]
        
        predicted_char = all_letters[top_i]
        predicted += predicted_char
        inp = string_to_tensor(predicted_char)

    return predicted

def train(inp, target, chunk_len):
    hidden = decoder.init_hidden()
    decoder.zero_grad()
    loss = 0

    for c in range(chunk_len):
        output, hidden = decoder(inp[c], hidden)
        loss += criterion(output, torch.tensor([target[c]]))

    loss.backward()
    decoder_optimizer.step()

    return loss.data.item() / chunk_len

def get_rnn_perplexity(chunk, chunk_len):
    hidden = decoder.init_hidden()
    decoder.zero_grad()
    loss = 0
    decoder.eval()

    inp = string_to_tensor(chunk[:-1])
    target = string_to_tensor(chunk[1:])

    for c in range(chunk_len-1):
        output, hidden = decoder(inp[c], hidden)
        loss += criterion(output, torch.tensor([target[c]]))

    return np.exp(loss.data.item() / chunk_len)

def start_pad(c):
    return '~' * c

def ngrams(c, text):
    text_padded = start_pad(c) + text
    ngrams = []
    
    for i in range(0, len(text_padded)-c):
        context = text_padded[i:i+c]
        char = text_padded[i+c]
        ngrams.append((context, char))
    
    return ngrams
    

def create_ngram_model(model_class, path, c=2, k=0):
    model = model_class(c, k)
    with open(path, encoding='utf-8', errors='ignore') as f:
        model.update(f.read())
    return model

def create_ngram_model_lines(model_class, path, c=2, k=0):
    model = model_class(c, k)
    with open(path, encoding='utf-8', errors='ignore') as f:
        for line in f:
            model.update(line.strip())
    return model

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers=1):
        super(RNN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        
        self.encoder = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)
        self.decoder = nn.Linear(hidden_size, output_size)
        self.drop = nn.Dropout()
    
    def forward(self, input, hidden):
        input = self.encoder(input.view(1, -1))
        output, hidden = self.gru(input.view(1, 1, -1), hidden)
        output = self.drop(output)
        output = self.decoder(output.view(1, -1))
        return output, hidden

    def init_hidden(self):
        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))



class NgramModel(object):
    ''' A basic n-gram model using add-k smoothing '''

    def __init__(self, c, k):
        self.c = c
        self.k = k
        self.vocab = set()
        self.ngrams = dict()

    def get_vocab(self):
        ''' Returns the set of characters in the vocab '''
        return self.vocab

    def update(self, text):
        ''' Updates the model n-grams based on text '''
        ngram_list = ngrams(self.c, text)
        for context, char in ngram_list:
            self.vocab.add(char)
            if context in self.ngrams.keys():
                if char in self.ngrams[context].keys():
                    self.ngrams[context][char] +=1
                else:
                    self.ngrams[context][char] = 1
            else:
                self.ngrams[context] = dict()
                self.ngrams[context][char] = 1

    def prob(self, context, char):
        ''' Returns the probability of char appearing after context '''
        
        if context not in self.ngrams.keys():
            return 1.0/len(self.vocab)
        else:
            total = sum(self.ngrams[context].values())
            try:
                return (self.ngrams[context][char] + self.k)/(total + len(self.vocab)*self.k)
            except:
                return (self.k)/(total + len(self.vocab)*self.k)

    def random_char(self, context):
        ''' Returns a random character based on the given context and the 
            n-grams learned by this model '''
        rand_num = random.random()
        cumulative_prob = 0
        for char in sorted(self.vocab):
            cumulative_prob += self.prob(context, char)
            if cumulative_prob >= rand_num:
                return char

    def random_text(self, length):
        ''' Returns text of the specified character length based on the
            n-grams learned by this model '''
        return_string = ''
        context = self.c * '~'
        for i in range(0, length):
            char = self.random_char(context)
            return_string += char
            context = context[1:] + char
            
        return return_string

    def perplexity(self, text):

        p = 0
        padded_text = start_pad(self.c) + text
        for i in range(0, len(text)):
            context = padded_text[i:i+self.c]
            char = text[i]
            x = self.prob(context, char)
            if x>0:
                p += np.log(1/x)
            else:
                return np.float('inf')
        p = p * (1/len(text))
        p = np.exp(p) 
        
        return p

# =============================================================================
# Uncomment below to train network 
# =============================================================================

# =============================================================================
# n_epochs = 3000
# print_every = 250
# hidden_size = 100
# n_layers = 2
# lr = 0.002
# append_loss = 250
# 
# 
# # text_string = get_text_string(read_location = '/content/gdrive/MyDrive/shakespeare.txt')
# text_string = get_text_string(read_location='Script_RushHour2.txt')
# print(text_string)
# file_len = len(text_string)
# 
# decoder = RNN(n_characters, hidden_size, n_characters, n_layers)
# decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)
# criterion = nn.CrossEntropyLoss()
# 
# all_losses = []
# epochs = []
# loss_avg = 0
# 
# for epoch in range(1, n_epochs + 1):
#     input, target = get_random_training_set(text_string, file_len, chunk_len=200)
#     loss = train(input, target, chunk_len=200)       
#     loss_avg += loss
# 
#     if epoch % print_every == 0:
#         print(evaluate(decoder, get_random_start_letter(), 100), '\n')
# 
#     if epoch % append_loss == 0:
#         all_losses.append(loss_avg / append_loss)
#         epochs.append(epoch)
#         loss_avg = 0
# 
# plt.figure()
# plt.plot(epochs, all_losses)
# plt.xlabel('Epochs')
# plt.ylabel('Training Loss')
# plt.title('Training Loss vs. Epochs')
# plt.show()
# =============================================================================

# =============================================================================
# torch.save(decoder.state_dict(), '/content/model_generate', _use_new_zipfile_serialization=False)
# =============================================================================



decoder = RNN(100, 100, 100, 2)
decoder.load_state_dict(torch.load('model_generate'))
for i in range(0, 10):
    print(evaluate(decoder, get_random_start_letter(), 100), '\n')

# =============================================================================
# m = create_ngram_model(NgramModel, 'Script_RushHour2.txt', c=3, k=.0001)
# print(m.random_text(250))
# print()
# pirates = get_text_string(read_location='Script_PiratesoftheCaribbean.txt')
# print(m.perplexity(pirates))
# print()
# good_will_hunting = get_text_string(read_location='Script_GoodWillHunting.txt')
# print(m.perplexity(good_will_hunting))
# print()
# gullivers_travels = get_text_string(read_location='gullivers_travels.txt')
# print(m.perplexity(gullivers_travels))
# =============================================================================

# pirates_chunk = random_chunk(pirates, len(pirates), chunk_len=10000)
# good_will_chunk = random_chunk(good_will_hunting, len(good_will_hunting), chunk_len=10000)
# gullivers_chunk = random_chunk(gullivers_travels, len(gullivers_travels), chunk_len=10000)
# =============================================================================
# pirates_chunk = pirates
# good_will_chunk = good_will_hunting
# gullivers_chunk = gullivers_travels
# print(get_rnn_perplexity(pirates_chunk, len(pirates_chunk)))
# print(get_rnn_perplexity(good_will_chunk, len(good_will_chunk)))
# print(get_rnn_perplexity(gullivers_chunk, len(gullivers_chunk)))
# =============================================================================

